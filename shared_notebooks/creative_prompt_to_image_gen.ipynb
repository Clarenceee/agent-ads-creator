{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "executionInfo": {
     "elapsed": 28,
     "status": "ok",
     "timestamp": 1746365501746,
     "user": {
      "displayName": "Wei Bing Chuah",
      "userId": "12139668370354715299"
     },
     "user_tz": -480
    },
    "id": "X0z3JVOpVcd5",
    "outputId": "aa64dd56-2409-46c0-85a9-fcaa0634e28f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nColab Notebook: AI Imagery Pipeline - Phase 2: Creative Expert Agent & Image Generation\\n\\nThis notebook implements Steps 1-4 of the Phase 2 development plan:\\n1. Define Pydantic Model for Image Prompt (Structured Output)\\n2. Implement the \"Creative Expert\" Agent Function (`generate_image_prompt_for_strategy`)\\n   - Includes input refinement, reasoning, and handling missing branding/image reference.\\n3. Implement Prompt Assembly function (`assemble_final_prompt`)\\n   - Refined adaptive logic based on image reference/instructions.\\n4. Implement Image Generation/Editing function (`generate_image`) using the specified \"gpt-image-1\" model via OpenAI API.\\n   - Handles reference image input using client.images.edit.\\n   - Handles additional aspect ratios (2:3, 3:4) by mapping to closest supported size.\\n   - Uses a separate client/API key for image generation.\\n   - Fixed invalid parameter for image edit API call.\\n   - Added handling for b64_json response from edit API.\\n   - **Saves outputs (images and metadata JSON) to a timestamped run directory.**\\n\\nIt loads the JSON output from the previous phase, generates structured concepts,\\nassembles final text prompts, calls the image generation/editing API, and stores/displays results.\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Colab Notebook: AI Imagery Pipeline - Phase 2: Creative Expert Agent & Image Generation\n",
    "\n",
    "This notebook implements Steps 1-4 of the Phase 2 development plan:\n",
    "1. Define Pydantic Model for Image Prompt (Structured Output)\n",
    "2. Implement the \"Creative Expert\" Agent Function (`generate_image_prompt_for_strategy`)\n",
    "   - Includes input refinement, reasoning, and handling missing branding/image reference.\n",
    "3. Implement Prompt Assembly function (`assemble_final_prompt`)\n",
    "   - Refined adaptive logic based on image reference/instructions.\n",
    "4. Implement Image Generation/Editing function (`generate_image`) using the specified \"gpt-image-1\" model via OpenAI API.\n",
    "   - Handles reference image input using client.images.edit.\n",
    "   - Handles additional aspect ratios (2:3, 3:4) by mapping to closest supported size.\n",
    "   - Uses a separate client/API key for image generation.\n",
    "   - Fixed invalid parameter for image edit API call.\n",
    "   - Added handling for b64_json response from edit API.\n",
    "   - **Saves outputs (images and metadata JSON) to a timestamped run directory.**\n",
    "\n",
    "It loads the JSON output from the previous phase, generates structured concepts,\n",
    "assembles final text prompts, calls the image generation/editing API, and stores/displays results.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4543,
     "status": "ok",
     "timestamp": 1746365506621,
     "user": {
      "displayName": "Wei Bing Chuah",
      "userId": "12139668370354715299"
     },
     "user_tz": -480
    },
    "id": "kem8sRoyVe6J",
    "outputId": "629b18a3-adfa-47f0-f2c1-0e7861cf8a26"
   },
   "outputs": [],
   "source": [
    "# @title Setup: Install Libraries\n",
    "# !pip install openai pydantic instructor python-dotenv tenacity requests -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11157,
     "status": "ok",
     "timestamp": 1746365517782,
     "user": {
      "displayName": "Wei Bing Chuah",
      "userId": "12139668370354715299"
     },
     "user_tz": -480
    },
    "id": "P5_2FwofVja8",
    "outputId": "2bfe68be-d137-4ec9-a7bb-16cfa101b51d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported.\n"
     ]
    }
   ],
   "source": [
    "# @title Setup: Import Libraries\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "import traceback\n",
    "from typing import List, Optional, Dict, Any, Tuple # Added Tuple\n",
    "import base64 # Added for decoding b64_json\n",
    "import requests # ** Added for downloading images from URL **\n",
    "\n",
    "# Import necessary libraries for Colab, file handling, and API calls\n",
    "# from google.colab import drive\n",
    "from dotenv import load_dotenv\n",
    "from pydantic import BaseModel, Field, field_validator # Import Pydantic\n",
    "import instructor # Import instructor\n",
    "from IPython.display import Image as IPImage, display # For displaying images\n",
    "\n",
    "# Import OpenAI specific classes\n",
    "try:\n",
    "    from openai import OpenAI, APIConnectionError, RateLimitError, APIStatusError\n",
    "    from openai.types.chat import ChatCompletion # To access usage attribute type hints if needed\n",
    "    from openai.types.images_response import ImagesResponse # Type hint for image generation response\n",
    "except ImportError:\n",
    "    print(\"ERROR: openai library not found or old version. Please install with `pip install -U openai`\")\n",
    "    OpenAI = None # Set to None if import fails\n",
    "    ChatCompletion = None # Set to None if import fails\n",
    "    ImagesResponse = None # Set to None if import fails\n",
    "\n",
    "print(\"Libraries imported.\")\n",
    "if not OpenAI:\n",
    "    print(\"WARNING: OpenAI library import failed. API calls will not work.\")\n",
    "if not instructor:\n",
    "    print(\"WARNING: instructor library import failed. Structured LLM calls will not work.\")\n",
    "if not BaseModel:\n",
    "    print(\"WARNING: pydantic library import failed. Pydantic models cannot be defined.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3118,
     "status": "ok",
     "timestamp": 1746369797887,
     "user": {
      "displayName": "Wei Bing Chuah",
      "userId": "12139668370354715299"
     },
     "user_tz": -480
    },
    "id": "MJwoMhqoVrIL",
    "outputId": "072e38b5-2161-439f-f3e1-88f19d1b6414"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Upstream output directory found: D:\\Self-Project\\LLM\\agent_ads\\pipeline_upstream_outputs\n",
      "   Expecting input JSON (with prompts) at: D:\\Self-Project\\LLM\\agent_ads\\pipeline_upstream_outputs\\output_with_prompts_20250504_144134.json\n",
      "⚠️ Warning: Input JSON file with prompts not found at D:\\Self-Project\\LLM\\agent_ads\\pipeline_upstream_outputs\\output_with_prompts_20250504_144134.json.\n",
      "   Expecting reference images (if any) in: D:\\Self-Project\\LLM\\agent_ads\\pipeline_upstream_outputs\n",
      "✅ Downstream output directory found: D:\\Self-Project\\LLM\\agent_ads\\pipeline_donwstream_outputs\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "\n",
    "    # --- Define Base Path on Google Drive (primarily for .env file) ---\n",
    "    # ** IMPORTANT: Update this path if your .env file is elsewhere **\n",
    "    DRIVE_BASE_PATH = r'D:\\Self-Project\\LLM\\agent_ads' # <--- UPDATE IF NEEDED for .env\n",
    "\n",
    "    # --- Define Path for Pipeline Outputs ---\n",
    "    # Directory containing outputs from previous steps (JSON with strategies, reference image)\n",
    "    PIPELINE_UPSTREAM_OUTPUT_DIR = r'D:\\Self-Project\\LLM\\agent_ads\\pipeline_upstream_outputs' # <--- Specific output directory from Phase 1/Step 2\n",
    "    PIPELINE_DOWNSTREAM_DIR = r'D:\\Self-Project\\LLM\\agent_ads\\pipeline_donwstream_outputs' # <--- Base directory for final outputs\n",
    "\n",
    "    # Define path for the input JSON from the previous phase\n",
    "    # ** CORRECTED: Use the JSON file containing strategies from upstream **\n",
    "    INPUT_JSON_FILENAME_PHASE1 = 'output_20250504_142856.json' # <--- JSON file from Phase 1 (contains strategies)\n",
    "    INPUT_JSON_PATH_PHASE1 = os.path.join(PIPELINE_UPSTREAM_OUTPUT_DIR, INPUT_JSON_FILENAME_PHASE1)\n",
    "\n",
    "    # Define path for the input JSON from the previous phase\n",
    "    # This JSON contains the generated structured prompts from Step 2 run\n",
    "    INPUT_JSON_FILENAME_WITH_PROMPTS = 'output_with_prompts_20250504_144134.json' # <-- Specific JSON filename from previous cell run\n",
    "    INPUT_JSON_PATH_WITH_PROMPTS = os.path.join(PIPELINE_UPSTREAM_OUTPUT_DIR, INPUT_JSON_FILENAME_WITH_PROMPTS)\n",
    "\n",
    "    # Define path for the reference image folder (where original ref image is stored)\n",
    "    IMAGE_INPUT_DIR = PIPELINE_UPSTREAM_OUTPUT_DIR # Reference image is saved in the same upstream output directory\n",
    "\n",
    "    # --- Directory Checks ---\n",
    "    path_error = False\n",
    "    # Check upstream dir\n",
    "    if not os.path.isdir(PIPELINE_UPSTREAM_OUTPUT_DIR):\n",
    "        print(f\"❌ ERROR: Upstream output directory not found: {PIPELINE_UPSTREAM_OUTPUT_DIR}\")\n",
    "        PIPELINE_UPSTREAM_OUTPUT_DIR = None\n",
    "        INPUT_JSON_PATH_WITH_PROMPTS = None\n",
    "        IMAGE_INPUT_DIR = None\n",
    "        path_error = True\n",
    "    else:\n",
    "        print(f\"✅ Upstream output directory found: {PIPELINE_UPSTREAM_OUTPUT_DIR}\")\n",
    "        if INPUT_JSON_PATH_WITH_PROMPTS:\n",
    "             print(f\"   Expecting input JSON (with prompts) at: {INPUT_JSON_PATH_WITH_PROMPTS}\")\n",
    "             if not os.path.exists(INPUT_JSON_PATH_WITH_PROMPTS):\n",
    "                  print(f\"⚠️ Warning: Input JSON file with prompts not found at {INPUT_JSON_PATH_WITH_PROMPTS}.\")\n",
    "             else:\n",
    "                  print(f\"✅ Input JSON file with prompts found.\")\n",
    "        if IMAGE_INPUT_DIR:\n",
    "             print(f\"   Expecting reference images (if any) in: {IMAGE_INPUT_DIR}\")\n",
    "\n",
    "    # Check/Create downstream dir\n",
    "    if not os.path.isdir(PIPELINE_DOWNSTREAM_DIR):\n",
    "        print(f\"⚠️ Downstream output directory not found: {PIPELINE_DOWNSTREAM_DIR}. Attempting to create.\")\n",
    "        try:\n",
    "            os.makedirs(PIPELINE_DOWNSTREAM_DIR, exist_ok=True)\n",
    "            print(f\"✅ Created downstream output directory: {PIPELINE_DOWNSTREAM_DIR}\")\n",
    "        except Exception as mkdir_e:\n",
    "            print(f\"❌ Error creating downstream directory {PIPELINE_DOWNSTREAM_DIR}: {mkdir_e}\")\n",
    "            PIPELINE_DOWNSTREAM_DIR = None\n",
    "            path_error = True\n",
    "    else:\n",
    "         print(f\"✅ Downstream output directory found: {PIPELINE_DOWNSTREAM_DIR}\")\n",
    "\n",
    "    if path_error:\n",
    "        print(\"ERROR: Halting due to missing critical directories.\")\n",
    "        # Optional: raise an exception here to stop execution\n",
    "        # raise FileNotFoundError(\"Critical directories missing, cannot proceed.\")\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ An error occurred during Google Drive mounting or path setting: {e}\")\n",
    "    DRIVE_BASE_PATH = None # Still needed for .env logic below\n",
    "    PIPELINE_UPSTREAM_OUTPUT_DIR = None\n",
    "    PIPELINE_DOWNSTREAM_DIR = None\n",
    "    INPUT_JSON_PATH_WITH_PROMPTS = None\n",
    "    IMAGE_INPUT_DIR = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 176,
     "status": "ok",
     "timestamp": 1746369579028,
     "user": {
      "displayName": "Wei Bing Chuah",
      "userId": "12139668370354715299"
     },
     "user_tz": -480
    },
    "id": "G8izJs1UVtom",
    "outputId": "35cbecd5-e220-4a01-ffbd-37d66d1f30ad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded .env file from path: D:\\Self-Project\\LLM\\agent_ads\\.env_colab\n"
     ]
    }
   ],
   "source": [
    "# @title Setup: Load API Keys and Configure LLM Client\n",
    "# --- Load API Keys ---\n",
    "# Place a .env file in your DRIVE_BASE_PATH or a subfolder (e.g., 'secrets')\n",
    "# The .env file should contain:\n",
    "# OPENAI_API_KEY=your_openai_api_key_for_text_llm_here\n",
    "# IMAGE_GEN_API_KEY=your_openai_api_key_for_image_gen_here # <-- ADD THIS KEY FOR IMAGE MODEL\n",
    "# OPENROUTER_API_KEY=your_openrouter_api_key_here # Optional, if using OpenRouter\n",
    "\n",
    "# Look for .env in DRIVE_BASE_PATH (can be adjusted if needed)\n",
    "dotenv_path = os.path.join(DRIVE_BASE_PATH, \".env_colab\") if DRIVE_BASE_PATH else None\n",
    "\n",
    "if dotenv_path and os.path.exists(dotenv_path):\n",
    "    load_dotenv(dotenv_path=dotenv_path)\n",
    "    print(f\"✅ Loaded .env file from path: {dotenv_path}\")\n",
    "elif DRIVE_BASE_PATH: # Only warn if base path was set correctly\n",
    "    print(f\"⚠️ Warning: .env file not found at {dotenv_path} (relative to DRIVE_BASE_PATH). API keys should be set as environment variables.\")\n",
    "else: # Drive mount or base path failed\n",
    "    print(f\"⚠️ Warning: Cannot check for .env file as DRIVE_BASE_PATH is not set. API keys should be set as environment variables.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pricing = {\n",
    "    \"x-ai/grok-3-beta\": {\"input\": 3.00, \"output\": 15.00},\n",
    "    \"gemini-2.5-pro-exp-03-25\": {\"input\": 0.00, \"output\": 0.00},\n",
    "    \"google/gemini-2.5-pro-preview-03-25\": {\"input\": 1.25, \"output\": 10.00},\n",
    "    \"google/gemini-2.5-flash-preview\": {\"input\": 0.15, \"output\": 0.60},\n",
    "    \"openai/o4-mini\": {\"input\": 1.10, \"output\": 4.40},\n",
    "    \"openai/gpt-4.1-mini\": {\"input\": 0.40, \"output\": 1.60},\n",
    "    \"gpt-image-1\": {\"input_text\": 5.00, \"input_img\": 10.00, \"output\": 40.00}\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configure LLM Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Instructor client configured.\n",
      "   - Creative Expert Model: openai/o4-mini\n",
      "✅ Image Generation client configured (using IMAGE_GEN_API_KEY).\n",
      "   - Image Generation Model: gpt-image-1\n"
     ]
    }
   ],
   "source": [
    "# OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "OPENROUTER_API_KEY = os.getenv(\"OPENROUTER_API_KEY\")\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY_1\")\n",
    "IMAGE_GEN_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# --- Configure LLM Client (Instructor + OpenAI) ---\n",
    "# We need an LLM client for the Creative Expert agent (Step 2)\n",
    "# We will use the OpenAI client patched with Instructor\n",
    "# Separate client for image generation (can use the same API key)\n",
    "instructor_client = None\n",
    "image_client = None\n",
    "\n",
    "# Set max retries for LLM calls (e.g., 3 attempts)\n",
    "MAX_LLM_RETRIES = 0 \n",
    "\n",
    "# Define the model to use for the Creative Expert agent\n",
    "# GPT-4o is recommended for its strong reasoning and potential vision capabilities if needed later\n",
    "# Select LLM service provider\n",
    "LLM_SERVICE_PROVIDER = \"OpenRouter\" # or \"Gemini\" or \"openai\" or \"OpenRouter\"\n",
    "\n",
    "CREATIVE_EXPERT_MODEL = \"openai/o4-mini\" # Or \"gemini-2.5-pro-exp-03-25\" \"openai/gpt-4.1-mini\" \"google/gemini-2.5-flash-preview\" \"google/gemini-2.5-pro-preview-03-25\" \"x-ai/grok-3-beta\" \"openai/o4-mini\"\n",
    "\n",
    "# ** MODIFIED: Define the Image Generation Model based on user documentation **\n",
    "IMAGE_GENERATION_MODEL = \"gpt-image-1\" # Using identifier provided by user documentation\n",
    "\n",
    "\n",
    "if OpenAI and instructor: # Check for packages\n",
    "  if OPENROUTER_API_KEY or GEMINI_API_KEY:\n",
    "    if LLM_SERVICE_PROVIDER == \"OpenRouter\":\n",
    "      BASE_API_URL = \"https://openrouter.ai/api/v1\"\n",
    "      BASE_API_KEY = OPENROUTER_API_KEY\n",
    "    elif LLM_SERVICE_PROVIDER == \"Gemini\":\n",
    "      BASE_API_URL = \"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    "      BASE_API_KEY = GEMINI_API_KEY\n",
    "    try:\n",
    "        # Initialize the base OpenAI client\n",
    "        # Configure with retries using Tenacity settings integrated into the client\n",
    "        text_base_client = OpenAI(\n",
    "            api_key=BASE_API_KEY,\n",
    "            base_url=BASE_API_URL,\n",
    "            max_retries=MAX_LLM_RETRIES, # Configure retries directly in the client\n",
    "        )\n",
    "        # Patch the client with instructor\n",
    "        instructor_client = instructor.patch(text_base_client)\n",
    "        print(f\"✅ Instructor client configured.\")\n",
    "        print(f\"   - Creative Expert Model: {CREATIVE_EXPERT_MODEL}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error initializing Instructor client: {e}\")\n",
    "        instructor_client = None\n",
    "\n",
    "elif not OPENROUTER_API_KEY:\n",
    "      print(\"⚠️ OpenRouter API Key not found.\")\n",
    "      # OPENROUTER_API_KEY = input(\"Enter your OpenRouter API Key: \")\n",
    "elif not GEMINI_API_KEY:\n",
    "      print(\"⚠️ Gemini API Key not found.\")\n",
    "      # GEMINI_API_KEY = input(\"Enter your Gemini API Key: \")\n",
    "else: # Libraries missing\n",
    "     print(\"⚠️ OpenAI/Instructor library not available for Instructor client.\")\n",
    "\n",
    "\n",
    "if OpenAI and IMAGE_GEN_API_KEY:\n",
    "    try:\n",
    "        # Initialize a separate OpenAI client instance for image generation\n",
    "        image_client = OpenAI(\n",
    "            api_key=IMAGE_GEN_API_KEY,\n",
    "             # Retries can also be configured here if needed, separate from text client\n",
    "            max_retries=MAX_LLM_RETRIES\n",
    "        )\n",
    "        print(f\"✅ Image Generation client configured (using IMAGE_GEN_API_KEY).\")\n",
    "        print(f\"   - Image Generation Model: {IMAGE_GENERATION_MODEL}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error initializing Image Generation client: {e}\")\n",
    "        image_client = None\n",
    "elif not IMAGE_GEN_API_KEY:\n",
    "     print(\"⚠️ IMAGE_GEN_API_KEY not found for Image Generation client.\")\n",
    "else: # OpenAI library missing\n",
    "     print(\"⚠️ OpenAI library not available for Image Generation client.\")\n",
    "\n",
    "# Final Check\n",
    "if not instructor_client:\n",
    "    print(\"   LLM-dependent steps will fail.\")\n",
    "if not image_client:\n",
    "     print(\"   Image generation steps will fail.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pydantic Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 44,
     "status": "ok",
     "timestamp": 1746369585068,
     "user": {
      "displayName": "Wei Bing Chuah",
      "userId": "12139668370354715299"
     },
     "user_tz": -480
    },
    "id": "K4kZ12H6V4-Y",
    "outputId": "bcd34cda-67f3-46d3-8a3e-8eb4576cc660"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Pydantic models 'VisualConceptDetails', 'ImageGenerationPrompt', and 'MarketingGoalSetFinal' defined.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if BaseModel:\n",
    "    # Define a structured format for the visual concept\n",
    "    class VisualConceptDetails(BaseModel):\n",
    "        \"\"\"Detailed breakdown of the visual concept.\"\"\"\n",
    "        main_subject: str = Field(..., description=\"Detailed description of the primary subject(s) and their interaction within the scene.\")\n",
    "        composition_and_framing: str = Field(..., description=\"Description of the composition, camera angle, shot type (e.g., close-up, wide shot), and framing.\")\n",
    "        background_environment: str = Field(..., description=\"Description of the background, setting, or environment.\")\n",
    "        foreground_elements: Optional[str] = Field(None, description=\"Description of any significant foreground elements.\")\n",
    "        lighting_and_mood: str = Field(..., description=\"Description of the lighting style (e.g., natural, studio, dramatic) and the overall mood or atmosphere.\")\n",
    "        color_palette: str = Field(..., description=\"Description of the key colors, color harmony (e.g., analogous, complementary), and overall color tone.\")\n",
    "        visual_style: str = Field(..., description=\"Description of the artistic or visual style (e.g., photorealistic, illustration, graphic design, vintage). This should include key style descriptors.\")\n",
    "        # Dedicated fields for text/branding visuals\n",
    "        promotional_text_visuals: Optional[str] = Field(None, description=\"Description of how promotional text (from task_description) should be visualized, including content, style, font characteristics, and placement suggestions.\")\n",
    "        branding_visuals: Optional[str] = Field(None, description=\"Description of how branding elements (logo placeholders, taglines, specific brand fonts/colors mentioned in branding_elements input) should be visually incorporated.\")\n",
    "        texture_and_details: Optional[str] = Field(None, description=\"Specific notes on textures, materials, or fine details.\")\n",
    "        negative_elements: Optional[str] = Field(None, description=\"Specific elements or concepts to actively avoid in the image.\")\n",
    "        # Field for creative reasoning\n",
    "        creative_reasoning: Optional[str] = Field(None, description=\"Brief explanation connecting the key visual choices (style, mood, composition, subject focus) back to the marketing strategy (audience, niche, objective, voice) and user inputs.\")\n",
    "\n",
    "\n",
    "    # Main output model now contains the structured concept\n",
    "    class ImageGenerationPrompt(BaseModel):\n",
    "        \"\"\"\n",
    "        Structured prompt details generated by the Creative Expert agent,\n",
    "        containing a breakdown of the visual concept. This structure will be\n",
    "        processed later to create the final prompt for the text-to-image model.\n",
    "        \"\"\"\n",
    "        visual_concept: VisualConceptDetails = Field(..., description=\"The detailed, structured breakdown of the visual concept.\")\n",
    "        aspect_ratio: str = Field(..., description=\"The target aspect ratio string (e.g., '1:1', '9:16', '16:9').\")\n",
    "        source_strategy_index: Optional[int] = Field(None, description=\"Index linking back to the source marketing strategy in the input JSON.\")\n",
    "\n",
    "        @field_validator('aspect_ratio')\n",
    "        def check_aspect_ratio(cls, v):\n",
    "            # ** MODIFIED: Added 2:3 and 3:4 to validation **\n",
    "            valid_ratios = ['1:1', '9:16', '16:9', '2:3', '3:4']\n",
    "            if v not in valid_ratios:\n",
    "                # Allow flexibility but warn if not standard image generation ratio\n",
    "                print(f\"Warning: Aspect ratio '{v}' is not one of the expected values {valid_ratios}. Mapping to size might fail or use default.\")\n",
    "            return v\n",
    "\n",
    "    # Define the structure for a single marketing strategy (input to Creative Expert)\n",
    "    # This should match the output structure from the previous phase.\n",
    "    class MarketingGoalSetFinal(BaseModel):\n",
    "        \"\"\"Represents a complete set of marketing goals for a creative direction.\"\"\"\n",
    "        target_audience: str\n",
    "        target_niche: str\n",
    "        target_objective: str\n",
    "        target_voice: str\n",
    "\n",
    "    print(\"✅ Pydantic models 'VisualConceptDetails', 'ImageGenerationPrompt', and 'MarketingGoalSetFinal' defined.\")\n",
    "\n",
    "else:\n",
    "    VisualConceptDetails = None\n",
    "    ImageGenerationPrompt = None\n",
    "    MarketingGoalSetFinal = None\n",
    "    print(\"⚠️ Pydantic models cannot be defined because the library is not available.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 36,
     "status": "ok",
     "timestamp": 1746369587700,
     "user": {
      "displayName": "Wei Bing Chuah",
      "userId": "12139668370354715299"
     },
     "user_tz": -480
    },
    "id": "LumAC02CV9vk",
    "outputId": "1505a2cf-1a4a-4df9-a481-defbf8746cfa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 'Creative Expert' agent function 'generate_image_prompt_for_strategy' defined.\n"
     ]
    }
   ],
   "source": [
    "# @title Step 2: Implement the \"Creative Expert\" Agent Function\n",
    "\n",
    "# Define the core function to generate the image prompt\n",
    "def generate_image_prompt_for_strategy(\n",
    "    generated_json: Dict[str, Any],\n",
    "    strategy: Dict[str, Any],\n",
    "    strategy_index: int,\n",
    "    aspect_ratio: str,\n",
    "    llm_client: instructor.Instructor # Expecting the patched client\n",
    ") -> Optional[Tuple[Dict[str, Any], Optional[Dict[str, int]]]]: # Return type includes usage info\n",
    "    \"\"\"\n",
    "    Generates a structured visual concept for a specific marketing strategy\n",
    "    using the Creative Expert LLM agent. Includes input refinement and reasoning.\n",
    "\n",
    "    Args:\n",
    "        generated_json: The full JSON dictionary output from the previous phase.\n",
    "        strategy: A dictionary representing one marketing strategy\n",
    "                  (matching MarketingGoalSetFinal structure).\n",
    "        strategy_index: The index of this strategy in the original list.\n",
    "        aspect_ratio: The target aspect ratio string (e.g., \"1:1\").\n",
    "        llm_client: The initialized and patched Instructor client.\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing:\n",
    "          - A dictionary representing the generated ImageGenerationPrompt object, or None if an error occurs.\n",
    "          - A dictionary containing token usage info (`prompt_tokens`, `completion_tokens`), or None if unavailable/error.\n",
    "    \"\"\"\n",
    "    # Check for Pydantic models as well\n",
    "    if not llm_client or not ImageGenerationPrompt or not VisualConceptDetails:\n",
    "        print(\"Error: LLM client or Pydantic models not available for Creative Expert.\")\n",
    "        return None, None # Return None for both prompt data and usage\n",
    "\n",
    "    # --- Extract Context from Input JSON ---\n",
    "    request_details = generated_json.get(\"request_details\", {})\n",
    "    user_inputs = generated_json.get(\"user_inputs\", {})\n",
    "    processing_context = generated_json.get(\"processing_context\", {})\n",
    "\n",
    "    task_type = request_details.get(\"task_type\", \"N/A\")\n",
    "    platform_name = request_details.get(\"target_platform\", {}).get(\"name\", \"N/A\")\n",
    "\n",
    "    user_prompt_original = user_inputs.get(\"prompt\")\n",
    "    image_reference = user_inputs.get(\"image_reference\") # Dict or None\n",
    "    branding_elements = user_inputs.get(\"branding_elements\")\n",
    "    task_description = user_inputs.get(\"task_description\") # Contains potential text for graphics\n",
    "\n",
    "    image_analysis = processing_context.get(\"image_analysis_result\") # Dict or None\n",
    "\n",
    "    # --- Handle Image Reference Context ---\n",
    "    has_image_reference = image_reference is not None\n",
    "    image_instruction = image_reference.get(\"instruction\") if has_image_reference else None\n",
    "    # Get saved image path if reference exists\n",
    "    saved_image_filename = image_reference.get(\"saved_image_path\") if has_image_reference else None\n",
    "    image_subject_from_analysis = None\n",
    "    image_style_from_analysis = None # Placeholder for potential future analysis field\n",
    "\n",
    "    if has_image_reference and isinstance(image_analysis, dict):\n",
    "        image_subject_from_analysis = image_analysis.get(\"main_subject\")\n",
    "        # Example: if image_analysis included style:\n",
    "        # image_style_from_analysis = image_analysis.get(\"style_mood\")\n",
    "\n",
    "    # --- Construct Prompt for Creative Expert LLM ---\n",
    "\n",
    "    # REFINED System prompt with reasoning instruction\n",
    "    system_prompt = \"\"\"\n",
    "    You are an expert Creative Director and Digital Marketing Strategist specializing in F&B social media visuals.\n",
    "    Your task is to generate a highly detailed, creative, and effective *structured visual concept* based on the provided marketing strategy and context.\n",
    "    This structured concept will later be used to generate a prompt for a text-to-image generation model.\n",
    "\n",
    "    **Input Refinement:** Critically review the user's inputs (Original User Prompt Hint, Specific Task Content/Description, Branding Guidelines, Image Instruction). If any input is brief, vague, contains grammatical errors, or seems misaligned with the core strategy/task, interpret the user's likely intent, refine it, expand upon it creatively, and clearly explain your refined interpretation within the relevant structured output fields. Ensure the final concept is coherent and aligns with the marketing strategy and task type.\n",
    "\n",
    "    **Core Task:** Embody creativity, imagination, and a deep understanding of visual design principles (color, composition, lighting, typography), F&B marketing trends, and image generation capabilities. Generate diverse, high-impact visual concepts tailored to the specific task type and marketing goals. Fill in all the fields of the requested Pydantic JSON output format (`ImageGenerationPrompt` containing `VisualConceptDetails`). Be specific, descriptive, and justify design choices implicitly through the descriptions. The `main_subject` field should describe all key subjects and their interaction clearly. The `visual_style` field should comprehensively describe the overall aesthetic, including relevant style descriptors.\n",
    "\n",
    "    **Handling Image Reference (CRITICAL):**\n",
    "    - If an image reference IS provided AND a specific user `instruction` IS given: Interpret the instruction and apply it when describing the concept (e.g., describe the style in `visual_style`, describe the subject in `main_subject`, describe a new background in `background_environment`).\n",
    "    - If an image reference IS provided BUT NO specific user `instruction` is given: The **primary subject** of the visual concept MUST be the analyzed subject from the reference image. Your main creative task is to design the *context* around this subject (composition, background, lighting, style, etc.) aligned with the marketing strategy. The `main_subject` field in your output MUST start with a detailed description of this reference subject, followed by its interaction with the scene you design.\n",
    "    - If NO image reference is provided: Generate the entire visual concept based on the marketing strategy and other inputs.\n",
    "\n",
    "    **Text & Branding:**\n",
    "    - If the task requires text (e.g., promotions, recipes, tips) or text is mentioned in the `task_description`, populate the `promotional_text_visuals` field. Describe the refined text content, its visual style (e.g., headline, caption), suggested font characteristics (e.g., bold sans-serif, elegant script), placement hierarchy, and integration with the overall visual.\n",
    "    - Analyze the `Branding Guidelines` input. If provided, describe how these elements (logo placeholders, taglines, specific brand fonts/colors) should be visually incorporated in the `branding_visuals` field. Refine branding input if necessary for clarity.\n",
    "    - **If `Branding Guidelines` are NOT provided, state this clearly in the `branding_visuals` field (e.g., \"No specific branding guidelines provided; visual style derived from strategy and task.\") and derive the visual style, colors, etc., primarily from the marketing strategy and task type.**\n",
    "\n",
    "    **Creative Reasoning:** After defining the visual concept, provide a brief explanation in the `creative_reasoning` field, connecting the key visual choices (style, mood, composition, subject focus, color palette) back to the core marketing strategy (audience, niche, objective, voice) and any significant user inputs or refinements made, especially noting how the image reference was handled.\n",
    "\n",
    "    Adhere strictly to the requested Pydantic JSON output format. Ensure all descriptions are detailed enough to guide image generation effectively.\n",
    "    \"\"\"\n",
    "\n",
    "    # User prompt asks for structured output\n",
    "    user_prompt_parts = [\n",
    "        f\"Generate a structured visual concept for an image targeting the '{platform_name}' platform ({aspect_ratio}).\",\n",
    "        f\"The core marketing strategy for this image is:\",\n",
    "        f\"- Target Audience: {strategy.get('target_audience', 'N/A')}\",\n",
    "        f\"- Target Niche: {strategy.get('target_niche', 'N/A')}\",\n",
    "        f\"- Target Objective: {strategy.get('target_objective', 'N/A')}\",\n",
    "        f\"- Target Voice: {strategy.get('target_voice', 'N/A')}\",\n",
    "        f\"\\nConsider the overall task context:\",\n",
    "        f\"- Task Type: {task_type}\",\n",
    "    ]\n",
    "\n",
    "    # Add notes about potential refinement\n",
    "    if user_prompt_original:\n",
    "        user_prompt_parts.append(f\"- Original User Prompt Hint: '{user_prompt_original}' (Interpret and refine this hint if it's brief or unclear, integrating its essence into the concept).\")\n",
    "    if task_description:\n",
    "        user_prompt_parts.append(f\"- Specific Task Content/Description: '{task_description}' (Interpret and refine this content if brief or unclear. If it includes text to be displayed, describe its visualization in `promotional_text_visuals`).\")\n",
    "    if branding_elements:\n",
    "        user_prompt_parts.append(f\"- Branding Guidelines: '{branding_elements}' (Interpret and refine these guidelines if brief or unclear. Describe their visualization in `branding_visuals`).\")\n",
    "    else:\n",
    "        # Explicitly note if branding is missing for the LLM's context\n",
    "        user_prompt_parts.append(\"- Branding Guidelines: Not Provided (Derive style from strategy/task).\")\n",
    "\n",
    "\n",
    "    # Image Reference Logic Integration into Prompt Instructions\n",
    "    user_prompt_parts.append(\"\\nImage Reference Context:\")\n",
    "    if has_image_reference:\n",
    "        user_prompt_parts.append(f\"- An image reference was provided (Filename: {saved_image_filename}).\") # Mention filename for context\n",
    "        if image_subject_from_analysis:\n",
    "             user_prompt_parts.append(f\"- Analysis identified the main subject as: '{image_subject_from_analysis}'.\")\n",
    "        # Add more analysis details if available (style, setting etc.)\n",
    "        # if image_style_from_analysis:\n",
    "        #     user_prompt_parts.append(f\"- Analysis suggested style/mood: '{image_style_from_analysis}'.\")\n",
    "\n",
    "        if image_instruction:\n",
    "            user_prompt_parts.append(f\"- User Instruction for reference image: '{image_instruction}' (Interpret and refine this instruction if brief or unclear). Apply the refined instruction carefully when describing the visual concept fields for the *new* image. For example, if asked to 'use the style', describe that style in the `visual_style` field. If asked to 'use the subject', describe that subject in the `main_subject` field. If asked to 'replace background', describe the original subject in `main_subject` but describe a new background in `background_environment`.\")\n",
    "        else:\n",
    "            # ** REFINED: Default behavior instruction strengthened **\n",
    "            user_prompt_parts.append(f\"- No specific instruction provided for the reference image. **Default behavior: The primary subject MUST be the analyzed subject ('{image_subject_from_analysis or 'Unknown'}').** Describe this subject in detail within the `main_subject` field, then focus your creative effort on designing the surrounding context (composition, background, lighting, style, etc.) to align with the marketing strategy.\")\n",
    "    else:\n",
    "        user_prompt_parts.append(\"- No image reference was provided.\")\n",
    "\n",
    "    # Final instruction asks for structured fields including creative reasoning\n",
    "    user_prompt_parts.append(f\"\"\"\n",
    "\\nBased on ALL the above context and your expertise (refining user inputs as needed), generate the `ImageGenerationPrompt` JSON object, ensuring the nested `VisualConceptDetails` object is fully populated with rich, descriptive details suitable for guiding a text-to-image model.\n",
    "- Describe the `main_subject` clearly (following image reference logic if applicable). This field should encompass all key subjects in the scene and their interactions.\n",
    "- Detail the `composition_and_framing`, including camera angle and shot type.\n",
    "- Describe the `background_environment` or setting.\n",
    "- Mention any important `foreground_elements`.\n",
    "- Specify the `lighting_and_mood`.\n",
    "- Define the `color_palette` and color harmony.\n",
    "- Articulate the `visual_style` (e.g., photorealistic, illustration, graphic design). Ensure this field comprehensively describes the desired aesthetic.\n",
    "- **Describe any required promotional text visualization in `promotional_text_visuals`.**\n",
    "- **Describe any required branding visualization (logos, taglines, brand fonts/colors) in `branding_visuals`. Handle the case where no branding guidelines were provided.**\n",
    "- Add notes on `texture_and_details` if relevant.\n",
    "- List any `negative_elements` to avoid.\n",
    "- **Provide a brief `creative_reasoning` explaining how the main visual choices connect to the marketing strategy and user inputs.**\n",
    "\n",
    "Ensure the overall visual concept aligns strongly with the marketing strategy, task type '{task_type}', and incorporates the image reference context as instructed above. Set `aspect_ratio` to '{aspect_ratio}'.\n",
    "\"\"\")\n",
    "\n",
    "    final_user_prompt = \"\\n\".join(user_prompt_parts)\n",
    "    print(f\"Final User Prompt : {final_user_prompt}\")\n",
    "    \n",
    "    # --- Make LLM Call ---\n",
    "    usage_info = None # Initialize usage info\n",
    "    try:\n",
    "        print(f\"\\n--- Generating structured prompt for Strategy {strategy_index} ---\")\n",
    "        # print(f\"DEBUG: Sending User Prompt to Creative Expert:\\n{final_user_prompt}\") # Uncomment for debugging prompts\n",
    "\n",
    "        # Use the globally configured CREATIVE_EXPERT_MODEL\n",
    "        completion = llm_client.chat.completions.create(\n",
    "            model=CREATIVE_EXPERT_MODEL,\n",
    "            response_model=ImageGenerationPrompt, # Request the main model\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": final_user_prompt}\n",
    "            ],\n",
    "            temperature=0.7, # Allow for creativity\n",
    "            max_tokens=2300, # Increase tokens slightly more for reasoning field\n",
    "        )\n",
    "\n",
    "        # Add the strategy index to the result before returning\n",
    "        prompt_data = completion.model_dump()\n",
    "        prompt_data['source_strategy_index'] = strategy_index\n",
    "\n",
    "        print(f\"✅ Successfully generated structured prompt object for Strategy {strategy_index}.\")\n",
    "        # print(f\"   Prompt Details: {prompt_data}\") # Uncomment for detailed output\n",
    "\n",
    "        # ** ADDED: Extract token usage **\n",
    "        try:\n",
    "            # Access the raw response potentially wrapped by instructor\n",
    "            raw_response = getattr(completion, '_raw_response', None)\n",
    "            if raw_response and hasattr(raw_response, 'usage') and raw_response.usage:\n",
    "                usage_data = raw_response.usage\n",
    "                usage_info = {\n",
    "                    \"prompt_tokens\": usage_data.prompt_tokens,\n",
    "                    \"completion_tokens\": usage_data.completion_tokens,\n",
    "                    \"total_tokens\": usage_data.total_tokens,\n",
    "                }\n",
    "                print(f\"  Token Usage (Strategy {strategy_index}): Input={usage_info['prompt_tokens']}, Output={usage_info['completion_tokens']}, Total={usage_info['total_tokens']}\")\n",
    "            else:\n",
    "                 print(\"  Token usage data not available in response object.\")\n",
    "        except Exception as usage_ex:\n",
    "            print(f\"  Warning: Could not extract token usage - {usage_ex}\")\n",
    "\n",
    "\n",
    "        return prompt_data, usage_info # Return both prompt and usage\n",
    "\n",
    "    except (APIConnectionError, RateLimitError, APIStatusError) as api_error:\n",
    "        print(f\"❌ ERROR: API call failed for Strategy {strategy_index}: {api_error}\")\n",
    "        return None, None # Return None for both\n",
    "    except Exception as e:\n",
    "        # Catch potential Pydantic validation errors or other issues\n",
    "        print(f\"❌ ERROR: Failed to generate or validate prompt for Strategy {strategy_index}: {e}\")\n",
    "        print(traceback.format_exc())\n",
    "        return None, None # Return None for both\n",
    "\n",
    "print(\"✅ 'Creative Expert' agent function 'generate_image_prompt_for_strategy' defined.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 103161,
     "status": "ok",
     "timestamp": 1746369694990,
     "user": {
      "displayName": "Wei Bing Chuah",
      "userId": "12139668370354715299"
     },
     "user_tz": -480
    },
    "id": "VSNr-u8UVZZK",
    "outputId": "fdf149ba-3124-4755-bceb-fa304a77f76d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully loaded Phase 1 input JSON from: D:\\Self-Project\\LLM\\agent_ads\\pipeline_upstream_outputs\\output_20250504_142856.json\n",
      "\n",
      "Found 5 marketing strategies. Generating structured prompts...\n",
      "\n",
      "--- Generating structured prompt for Strategy 0 ---\n",
      "✅ Successfully generated structured prompt object for Strategy 0.\n",
      "  Token Usage (Strategy 0): Input=1903, Output=1445, Total=3348\n",
      "\n",
      "--- Generating structured prompt for Strategy 1 ---\n",
      "✅ Successfully generated structured prompt object for Strategy 1.\n",
      "  Token Usage (Strategy 1): Input=1910, Output=1714, Total=3624\n",
      "\n",
      "--- Generating structured prompt for Strategy 2 ---\n",
      "✅ Successfully generated structured prompt object for Strategy 2.\n",
      "  Token Usage (Strategy 2): Input=1905, Output=994, Total=2899\n",
      "\n",
      "--- Generating structured prompt for Strategy 3 ---\n",
      "✅ Successfully generated structured prompt object for Strategy 3.\n",
      "  Token Usage (Strategy 3): Input=1904, Output=1073, Total=2977\n",
      "\n",
      "--- Generating structured prompt for Strategy 4 ---\n",
      "✅ Successfully generated structured prompt object for Strategy 4.\n",
      "  Token Usage (Strategy 4): Input=1904, Output=1220, Total=3124\n",
      "\n",
      "--- Prompt Generation Complete ---\n",
      "Successfully generated 5 structured prompts.\n",
      "✅ Stored generated prompts in pipeline_data_phase1['processing_context']['generated_image_prompts']\n",
      "\n",
      "--- Token Usage & Cost Estimation (Creative Expert Stage) ---\n",
      "Model Used: openai/o4-mini\n",
      "Total Input Tokens: 9526\n",
      "Total Output Tokens: 6446\n",
      "Total Tokens: 15972\n",
      "Estimated Cost: $0.038841\n",
      "(Based on openai/o4-mini pricing: $1.1/1M input, $4.41M output. Please verify current pricing.)\n",
      "\n",
      "Generated Structured Prompt Objects (showing first one as example):\n",
      "\n",
      "--- Structured Prompt for Strategy 0 ---\n",
      "{\n",
      "  \"visual_concept\": {\n",
      "    \"main_subject\": \"A steaming bowl of stir-fried noodles with vibrant vegetables\\u2014bright green bok choy, red bell pepper strips, julienned carrots, and crisp snow peas\\u2014tossed in a glossy savory sauce, sprinkled with sesame seeds, and garnished with fresh cilantro sprigs and red chili flakes. A pair of sleek black lacquered chopsticks lifts a twisted mouthful of noodles, emphasizing texture and freshness.\",\n",
      "    \"composition_and_framing\": \"Square (1:1) frame with a 30\\u00b0 overhead angle, centering the bowl slightly to the right following the rule of thirds. The chopsticks create a dynamic diagonal line leading the eye. Negative space on the left accommodates promotional text.\",\n",
      "    \"background_environment\": \"Minimalist light wood tabletop with subtle bamboo mat texture. A few blurred elements suggest an authentic Asian kitchen setting, but remain unobtrusive.\",\n",
      "    \"foreground_elements\": \"A small dish of chili oil and scattered fresh ingredients (garlic cloves, red chilies, sesame seeds) lightly blurred at the bottom left corner to add context without distracting from the main subject.\",\n",
      "    \"lighting_and_mood\": \"Bright, natural diffused light from the top-left, creating soft shadows and highlights that accentuate the noodles\\u2019 glossy sheen and vibrant vegetable colors. The overall mood is fresh, energetic, and inviting.\",\n",
      "    \"color_palette\": \"Warm golden yellows and rich browns of the noodles and sauce, complemented by pops of green and red from vegetables. Neutral wood tones and soft ivory negative space. High saturation with complementary accents.\",\n",
      "    \"visual_style\": \"Photorealistic product photography with shallow depth-of-field. Crisp focus on the bowl and lifted noodles, gentle bokeh background to draw attention to texture and details.\",\n",
      "    \"promotional_text_visuals\": \"Headline: \\u201cTaste Authentic Asian Flavors\\u201d in bold, uppercase sans-serif (white with subtle drop shadow) placed in the left negative space. Subheadline: \\u201cStir-Fried Veggie Noodles\\u201d in smaller italicized script font beneath. Use brand-accent yellow for key words. Ensure text hierarchy with the headline largest, subheadline medium.\",\n",
      "    \"branding_visuals\": \"No specific branding guidelines provided; include a semi-transparent white circular logo placeholder at bottom right. Below the logo, place the tagline \\u201cAuthentic Asian Delights\\u201d in a clean sans-serif, small caps, brand-accent yellow.\",\n",
      "    \"texture_and_details\": \"Emphasize the glossy sauce coating, sesame seed texture, and moisture droplets on vegetables. Capture the wood grain and bamboo mat weave subtly in focus range.\",\n",
      "    \"negative_elements\": \"Avoid cluttered props, overly dark or moody lighting, greasy or oily sheen, and any non-Asian cultural artifacts or text.\",\n",
      "    \"creative_reasoning\": \"The overhead angle and shallow depth-of-field align with product photography trends on Instagram, focusing on texture and freshness for foodies. Vibrant complementary colors and natural lighting reinforce an authentic, culturally rich voice. Negative space accommodates promotional text without detracting from the dish, enhancing brand awareness and engagement.\"\n",
      "  },\n",
      "  \"aspect_ratio\": \"1:1\",\n",
      "  \"source_strategy_index\": 0\n",
      "}\n",
      "\n",
      "...(plus 4 more)\n",
      "\n",
      "✅ Successfully saved updated pipeline data with prompts to: D:\\Self-Project\\LLM\\agent_ads\\pipeline_upstream_outputs\\output_with_prompts_20250506_213929.json\n"
     ]
    }
   ],
   "source": [
    "# @title Example Usage: Load JSON and Generate Prompts\n",
    "\n",
    "# Ensure Pydantic models and function are defined\n",
    "if ImageGenerationPrompt and MarketingGoalSetFinal and 'generate_image_prompt_for_strategy' in globals():\n",
    "\n",
    "    # Load the input JSON data\n",
    "    pipeline_data_phase1 = None # Use a distinct variable name\n",
    "    generated_prompts = [] # List to store successfully generated prompt dicts\n",
    "    total_prompt_tokens = 0 # Initialize token counters\n",
    "    total_completion_tokens = 0\n",
    "    total_tokens = 0\n",
    "\n",
    "    # Check if INPUT_JSON_PATH was set correctly\n",
    "    if INPUT_JSON_PATH_PHASE1 and os.path.exists(INPUT_JSON_PATH_PHASE1):\n",
    "        try:\n",
    "            with open(INPUT_JSON_PATH_PHASE1, 'r') as f:\n",
    "                pipeline_data_phase1 = json.load(f)\n",
    "            print(f\"✅ Successfully loaded Phase 1 input JSON from: {INPUT_JSON_PATH_PHASE1}\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error loading JSON file '{INPUT_JSON_PATH_PHASE1}': {e}\")\n",
    "            pipeline_data_phase1 = None\n",
    "    elif INPUT_JSON_PATH_PHASE1: # Path was defined but file doesn't exist\n",
    "        print(f\"❌ Phase 1 Input JSON file not found at '{INPUT_JSON_PATH_PHASE1}'. Cannot proceed with example.\")\n",
    "    else: # Path wasn't even defined (e.g., Drive mount failed or PIPELINE_OUTPUT_DIR check failed)\n",
    "         print(f\"❌ Phase 1 Input JSON path not defined or accessible. Cannot proceed with example.\")\n",
    "\n",
    "\n",
    "    # Proceed only if JSON loaded and LLM client is ready\n",
    "    if pipeline_data_phase1 and instructor_client:\n",
    "        # Ensure 'processing_context' exists before trying to access it\n",
    "        if \"processing_context\" not in pipeline_data_phase1:\n",
    "            pipeline_data_phase1[\"processing_context\"] = {}\n",
    "            print(\"⚠️ Initialized empty 'processing_context' in loaded data.\")\n",
    "\n",
    "        strategies = pipeline_data_phase1.get(\"processing_context\", {}).get(\"suggested_marketing_strategies\", [])\n",
    "        target_platform_info = pipeline_data_phase1.get(\"request_details\", {}).get(\"target_platform\", {})\n",
    "        aspect_ratio = target_platform_info.get(\"resolution\", {}).get(\"aspect_ratio\", \"1:1\") # Default to 1:1 if missing\n",
    "\n",
    "        if not strategies:\n",
    "            print(\"⚠️ No suggested marketing strategies found in the input JSON.\")\n",
    "        else:\n",
    "            print(f\"\\nFound {len(strategies)} marketing strategies. Generating structured prompts...\")\n",
    "\n",
    "            # Iterate through each strategy and generate a prompt\n",
    "            for index, strategy_dict in enumerate(strategies):\n",
    "                # Validate the strategy dict structure (optional but good practice)\n",
    "                try:\n",
    "                    validated_strategy = MarketingGoalSetFinal(**strategy_dict)\n",
    "                except Exception as val_err:\n",
    "                    print(f\"⚠️ Warning: Skipping strategy {index} due to invalid format: {val_err}\")\n",
    "                    continue\n",
    "\n",
    "                # ** MODIFIED: Capture prompt data and usage info **\n",
    "                prompt_result_dict, usage_info = generate_image_prompt_for_strategy(\n",
    "                    generated_json=pipeline_data_phase1, # Use Phase 1 data\n",
    "                    strategy=validated_strategy.model_dump(), # Pass strategy as dict\n",
    "                    strategy_index=index,\n",
    "                    aspect_ratio=aspect_ratio,\n",
    "                    llm_client=instructor_client\n",
    "                )\n",
    "                time.sleep(1)\n",
    "\n",
    "                if prompt_result_dict:\n",
    "                    generated_prompts.append(prompt_result_dict)\n",
    "                    # ** ADDED: Aggregate token usage **\n",
    "                    if usage_info:\n",
    "                        total_prompt_tokens += usage_info.get(\"prompt_tokens\", 0)\n",
    "                        total_completion_tokens += usage_info.get(\"completion_tokens\", 0)\n",
    "                        total_tokens += usage_info.get(\"total_tokens\", 0)\n",
    "                else:\n",
    "                    # Log failure for this specific strategy\n",
    "                    print(f\"--- Failed to generate structured prompt for Strategy {index} ---\")\n",
    "\n",
    "            print(\"\\n--- Prompt Generation Complete ---\")\n",
    "            print(f\"Successfully generated {len(generated_prompts)} structured prompts.\")\n",
    "\n",
    "            # Store generated prompts back into the main data structure\n",
    "            pipeline_data_phase1[\"processing_context\"][\"generated_image_prompts\"] = generated_prompts\n",
    "            print(f\"✅ Stored generated prompts in pipeline_data_phase1['processing_context']['generated_image_prompts']\")\n",
    "\n",
    "            # ** ADDED: Display token usage and estimated cost **\n",
    "            print(\"\\n--- Token Usage & Cost Estimation (Creative Expert Stage) ---\")\n",
    "            print(f\"Model Used: {CREATIVE_EXPERT_MODEL}\")\n",
    "            print(f\"Total Input Tokens: {total_prompt_tokens}\")\n",
    "            print(f\"Total Output Tokens: {total_completion_tokens}\")\n",
    "            print(f\"Total Tokens: {total_tokens}\")\n",
    "\n",
    "            # --- Pricing (Example for GPT-4o as of May 2024 - CHECK CURRENT PRICING) ---\n",
    "            # Input: $5.00 / 1M tokens\n",
    "            # Output: $15.00 / 1M tokens\n",
    "            # NOTE: Pricing can change frequently. Verify current rates on OpenAI's website.\n",
    "\n",
    "            try:\n",
    "                price_per_million_input = model_pricing[CREATIVE_EXPERT_MODEL][\"input\"] if CREATIVE_EXPERT_MODEL in model_pricing else 0.0\n",
    "                price_per_million_output = model_pricing[CREATIVE_EXPERT_MODEL][\"output\"] if CREATIVE_EXPERT_MODEL in model_pricing else 0.0\n",
    "                estimated_cost = ((total_prompt_tokens / 1_000_000) * price_per_million_input) + \\\n",
    "                                 ((total_completion_tokens / 1_000_000) * price_per_million_output)\n",
    "                print(f\"Estimated Cost: ${estimated_cost:.6f}\")\n",
    "                if CREATIVE_EXPERT_MODEL in model_pricing:\n",
    "                    print(f\"(Based on {CREATIVE_EXPERT_MODEL} pricing: ${price_per_million_input}/1M input, ${price_per_million_output}1M output. Please verify current pricing.)\")\n",
    "                else:\n",
    "                    print(f\"(No pricing information available for {CREATIVE_EXPERT_MODEL}).\")\n",
    "            except Exception as cost_e:\n",
    "                print(f\"Could not calculate cost: {cost_e}\")\n",
    "            # --- End Cost Estimation ---\n",
    "\n",
    "\n",
    "            # Display the generated prompts (optional)\n",
    "            if generated_prompts:\n",
    "                 print(\"\\nGenerated Structured Prompt Objects (showing first one as example):\")\n",
    "                 if generated_prompts: # Check again in case generation failed for all\n",
    "                     print(f\"\\n--- Structured Prompt for Strategy {generated_prompts[0].get('source_strategy_index', 0)} ---\")\n",
    "                     # Pretty print the dictionary\n",
    "                     print(json.dumps(generated_prompts[0], indent=2))\n",
    "                     if len(generated_prompts) > 1:\n",
    "                         print(f\"\\n...(plus {len(generated_prompts) - 1} more)\")\n",
    "                 else:\n",
    "                     print(\"No prompts were successfully generated.\")\n",
    "\n",
    "\n",
    "            # Option to save the updated JSON data\n",
    "            save_output = True # Set to False to disable saving\n",
    "            if save_output and PIPELINE_UPSTREAM_OUTPUT_DIR: # Save to upstream dir for next step\n",
    "                try:\n",
    "                    # Create a new filename for the output with prompts\n",
    "                    output_filename = f\"output_with_prompts_{time.strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "                    output_path = os.path.join(PIPELINE_UPSTREAM_OUTPUT_DIR, output_filename)\n",
    "                    with open(output_path, 'w') as f:\n",
    "                        json.dump(pipeline_data_phase1, f, indent=2) # Save the updated phase 1 data\n",
    "                    print(f\"\\n✅ Successfully saved updated pipeline data with prompts to: {output_path}\")\n",
    "                    # ** Set this path for the next cell **\n",
    "                    INPUT_JSON_PATH_WITH_PROMPTS = output_path\n",
    "                except Exception as save_e:\n",
    "                    print(f\"\\n❌ Error saving updated JSON file: {save_e}\")\n",
    "\n",
    "\n",
    "    elif not pipeline_data_phase1:\n",
    "        # Message already printed above if file not found/path undefined\n",
    "        pass\n",
    "    else: # instructor_client is None\n",
    "         print(\"Example usage skipped because the LLM client is not configured.\")\n",
    "\n",
    "else:\n",
    "    print(\"⚠️ Example usage skipped because Pydantic models or the generation function are not defined (check setup steps).\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 100,
     "status": "ok",
     "timestamp": 1746369856203,
     "user": {
      "displayName": "Wei Bing Chuah",
      "userId": "12139668370354715299"
     },
     "user_tz": -480
    },
    "id": "iQ_3OjiLJ_i7",
    "outputId": "ca87c787-a8ee-4842-d0b4-81c9358554e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Prompt assembly and Image generation/editing functions defined.\n"
     ]
    }
   ],
   "source": [
    "# @title Step 3 & 4: Prompt Assembly and Image Generation Functions\n",
    "\n",
    "# Added user_inputs parameter and refined logic\n",
    "def assemble_final_prompt(structured_prompt_data: Dict[str, Any], user_inputs: Dict[str, Any]) -> str:\n",
    "    \"\"\"\n",
    "    Assembles the final text prompt string from the structured visual concept details,\n",
    "    adapting based on original user inputs for image reference.\n",
    "\n",
    "    Args:\n",
    "        structured_prompt_data: A dictionary representing the ImageGenerationPrompt object.\n",
    "        user_inputs: The user_inputs dictionary from the main pipeline data.\n",
    "\n",
    "    Returns:\n",
    "        A string suitable for input to the image generation API.\n",
    "    \"\"\"\n",
    "    if not structured_prompt_data or \"visual_concept\" not in structured_prompt_data:\n",
    "        return \"Error: Invalid structured prompt data.\"\n",
    "\n",
    "    vc = structured_prompt_data[\"visual_concept\"]\n",
    "    aspect_ratio = structured_prompt_data.get(\"aspect_ratio\", \"1:1\") # Get aspect ratio for context\n",
    "\n",
    "    # Check original user inputs for image reference context\n",
    "    image_reference = user_inputs.get(\"image_reference\")\n",
    "    has_reference = image_reference is not None\n",
    "    has_instruction = has_reference and image_reference.get(\"instruction\")\n",
    "    instruction_text = image_reference.get(\"instruction\", \"\") if has_instruction else \"\"\n",
    "\n",
    "    # --- Assemble Core Description ---\n",
    "    # Combine the detailed descriptions generated by the LLM\n",
    "    core_description_parts = [\n",
    "        vc.get(\"main_subject\"),\n",
    "        vc.get(\"composition_and_framing\"),\n",
    "        f\"Background: {vc.get('background_environment')}\",\n",
    "        f\"Foreground elements: {vc.get('foreground_elements')}\" if vc.get(\"foreground_elements\") else None,\n",
    "        f\"Lighting & Mood: {vc.get('lighting_and_mood')}\",\n",
    "        f\"Color Palette: {vc.get('color_palette')}\",\n",
    "        f\"Visual Style: {vc.get('visual_style')}\",\n",
    "        f\"Textures & Details: {vc.get('texture_and_details')}\" if vc.get(\"texture_and_details\") else None,\n",
    "        f\"Text Visualization: {vc.get('promotional_text_visuals')}\" if vc.get(\"promotional_text_visuals\") else None,\n",
    "        f\"Branding Visualization: {vc.get('branding_visuals')}\" if vc.get(\"branding_visuals\") else None,\n",
    "        f\"Avoid the following elements: {vc.get('negative_elements')}\" if vc.get(\"negative_elements\") else None,\n",
    "    ]\n",
    "    core_description = \". \".join(filter(None, core_description_parts))\n",
    "\n",
    "    # --- Add Contextual Prefix based on Reference/Instruction ---\n",
    "    # This prefix provides clearer intent to the image model when editing vs generating\n",
    "    prefix = \"\"\n",
    "    if has_reference:\n",
    "        if has_instruction:\n",
    "            # If editing based on specific instruction\n",
    "             prefix = f\"Based on the provided reference image, modify it according to the user instruction '{instruction_text}' to achieve the following visual concept: \"\n",
    "        else:\n",
    "            # If editing using reference subject as primary (default)\n",
    "             prefix = f\"Using the primary subject from the provided reference image, create a new image with the following visual concept: \"\n",
    "    # else: No prefix needed for pure generation\n",
    "\n",
    "    # --- Combine Prefix and Core Description ---\n",
    "    final_prompt = f\"{prefix}{core_description} Ensure the image strictly adheres to a {aspect_ratio} aspect ratio.\"\n",
    "\n",
    "    # print(f\"DEBUG: Assembled Prompt:\\n{final_prompt}\\n\") # Uncomment for debugging\n",
    "    return final_prompt\n",
    "\n",
    "\n",
    "def map_aspect_ratio_to_size(aspect_ratio: str) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Maps aspect ratio string to size parameter supported by OpenAI Images API\n",
    "    (e.g., DALL-E 3 sizes). Returns None if ratio is unsupported by standard sizes.\n",
    "    \"\"\"\n",
    "    # Added 2:3 and 3:4 mapping\n",
    "    if aspect_ratio == \"9:16\":\n",
    "        return \"1024x1792\"\n",
    "    elif aspect_ratio == \"16:9\":\n",
    "        return \"1792x1024\"\n",
    "    elif aspect_ratio == \"1:1\":\n",
    "        return \"1024x1024\"\n",
    "    elif aspect_ratio == \"2:3\":\n",
    "        print(f\"Warning: Mapping aspect ratio '2:3' to closest supported vertical size '1024x1792' (approx 9:16).\")\n",
    "        return \"1024x1792\"\n",
    "    elif aspect_ratio == \"3:4\":\n",
    "        print(f\"Warning: Mapping aspect ratio '3:4' to closest supported vertical size '1024x1792' (approx 9:16).\")\n",
    "        return \"1024x1792\"\n",
    "    else:\n",
    "        print(f\"Warning: Unsupported aspect ratio '{aspect_ratio}' for standard sizes. Cannot map to size.\")\n",
    "        return None # Indicate unsupported ratio\n",
    "\n",
    "# @retry(stop=stop_after_attempt(MAX_LLM_RETRIES), wait=wait_exponential(multiplier=1, min=4, max=10)) # Optional: Add tenacity retry decorator for image gen\n",
    "# ** MODIFIED: Added run_directory and strategy_index args **\n",
    "def generate_image(\n",
    "    final_prompt: str,\n",
    "    aspect_ratio: str,\n",
    "    client: OpenAI, # Expecting the base OpenAI client\n",
    "    run_directory: str, # Target directory for saving edited images\n",
    "    strategy_index: int, # Index for naming saved images\n",
    "    reference_image_path: Optional[str] = None\n",
    ") -> Optional[Tuple[str, Optional[str]]]: # Returns (status, url_or_filepath)\n",
    "    \"\"\"\n",
    "    Generates or edits an image using the OpenAI Images API (model specified by IMAGE_GENERATION_MODEL)\n",
    "    via the specified client. Uses client.images.edit if reference_image_path is provided,\n",
    "    otherwise uses client.images.generate. Saves edited images to the run_directory.\n",
    "\n",
    "    Args:\n",
    "        final_prompt: The assembled text prompt string describing the desired outcome or edit.\n",
    "        aspect_ratio: The aspect ratio string ('1:1', '9:16', '16:9', '2:3', '3:4').\n",
    "        client: The initialized OpenAI client.\n",
    "        run_directory: The path to the directory where outputs for this run are saved.\n",
    "        strategy_index: The index of the current strategy (for filename).\n",
    "        reference_image_path: Optional path to the reference image for editing.\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing:\n",
    "          - status: \"success\" or \"error\"\n",
    "          - url_or_filepath: The image URL (for generation) or local file path (for edits)\n",
    "                             if successful, or an error message string.\n",
    "    \"\"\"\n",
    "    if not client:\n",
    "        return \"error\", \"Image generation client not available.\"\n",
    "    if not final_prompt or final_prompt.startswith(\"Error:\"):\n",
    "         return \"error\", f\"Invalid final prompt provided: {final_prompt}\"\n",
    "    if not run_directory or not os.path.isdir(run_directory):\n",
    "         return \"error\", f\"Invalid run_directory provided: {run_directory}\"\n",
    "\n",
    "    try:\n",
    "        # Map aspect ratio to size, handle potential None\n",
    "        image_size = map_aspect_ratio_to_size(aspect_ratio)\n",
    "        if not image_size:\n",
    "            return \"error\", f\"Unsupported aspect ratio '{aspect_ratio}' for image generation/editing.\"\n",
    "\n",
    "        response: Optional[ImagesResponse] = None # Initialize response\n",
    "        operation_type = \"generation\" # Default\n",
    "\n",
    "        # Conditional logic for generate vs edit\n",
    "        if reference_image_path and os.path.exists(reference_image_path):\n",
    "            operation_type = \"editing\"\n",
    "            print(f\"--- Calling Image Editing API ({IMAGE_GENERATION_MODEL}) with reference image ---\")\n",
    "            print(f\"   Reference Image: {reference_image_path}\")\n",
    "            # print(f\"   Edit Prompt: {final_prompt[:200]}...\") # Print start of prompt for debug\n",
    "            try:\n",
    "                with open(reference_image_path, \"rb\") as image_file:\n",
    "                    # ** FIXED: Removed response_format from edit call **\n",
    "                    # ** NOTE: Edit API defaults to b64_json response format **\n",
    "                    response = client.images.edit(\n",
    "                        model=IMAGE_GENERATION_MODEL, # Use the model specified in setup\n",
    "                        image=image_file,\n",
    "                        prompt=final_prompt, # Prompt describes the desired edit/final state\n",
    "                        n=1,\n",
    "                        size=image_size # Use mapped size\n",
    "                    )\n",
    "            except FileNotFoundError:\n",
    "                 return \"error\", f\"Reference image not found at path: {reference_image_path}\"\n",
    "            except Exception as file_err:\n",
    "                 return \"error\", f\"Error opening reference image: {file_err}\"\n",
    "\n",
    "        else:\n",
    "            operation_type = \"generation\"\n",
    "            if reference_image_path: # Path provided but file doesn't exist\n",
    "                 print(f\"⚠️ Warning: Reference image path provided but file not found: {reference_image_path}. Falling back to generation.\")\n",
    "            print(f\"--- Calling Image Generation API ({IMAGE_GENERATION_MODEL}) ---\")\n",
    "            # print(f\"   Generation Prompt: {final_prompt[:200]}...\") # Print start of prompt for debug\n",
    "\n",
    "            response = client.images.generate(\n",
    "                model=IMAGE_GENERATION_MODEL, # Use the model specified in setup\n",
    "                prompt=final_prompt,\n",
    "                size=image_size, # Use mapped size\n",
    "                quality=\"high\",  # or \"hd\" - check if supported by gpt-image-1 if different from DALL-E\n",
    "                n=1,\n",
    "                # style=\"vivid\" # 'style' parameter is specific to DALL-E 3, may not apply to gpt-image-1\n",
    "                # response_format=\"url\" # Request URL for generation\n",
    "            )\n",
    "\n",
    "        # --- Process Response (Handles different formats) ---\n",
    "        if response and response.data and len(response.data) > 0:\n",
    "            image_data = response.data[0]\n",
    "            if operation_type == \"generation\" and image_data.url:\n",
    "                image_url = image_data.url\n",
    "                print(f\"✅ Image generation successful.\")\n",
    "                return \"success\", image_url # Return URL for generated images\n",
    "            elif operation_type == \"editing\" and image_data.b64_json:\n",
    "                # Handle b64_json response from edit\n",
    "                print(f\"✅ Image editing successful (received base64 data).\")\n",
    "                try:\n",
    "                    image_bytes = base64.b64decode(image_data.b64_json)\n",
    "                    # ** MODIFIED: Save edited image to the run_directory **\n",
    "                    timestamp = time.strftime('%Y%m%d_%H%M%S')\n",
    "                    local_filename = f\"edited_image_strategy_{strategy_index}_{timestamp}.png\"\n",
    "                    local_filepath = os.path.join(run_directory, local_filename) # Save in run dir\n",
    "                    with open(local_filepath, \"wb\") as f:\n",
    "                        f.write(image_bytes)\n",
    "                    print(f\"   Saved edited image to: {local_filepath}\")\n",
    "                    return \"success\", local_filepath # Return the persistent local path\n",
    "                except Exception as decode_save_err:\n",
    "                    print(f\"❌ Error decoding/saving base64 image: {decode_save_err}\")\n",
    "                    return \"error\", f\"Error processing base64 response: {decode_save_err}\"\n",
    "            else:\n",
    "                # Handle unexpected response format\n",
    "                error_msg = f\"Image API response format mismatch for {operation_type}.\"\n",
    "                print(f\"❌ {error_msg}\")\n",
    "                revised_prompt = image_data.revised_prompt if hasattr(image_data, 'revised_prompt') and image_data.revised_prompt else \"N/A\"\n",
    "                error_msg += f\" Revised prompt (if available): {revised_prompt}\"\n",
    "                return \"error\", error_msg\n",
    "        else:\n",
    "            error_msg = \"Image API response did not contain expected data structure.\"\n",
    "            print(f\"❌ {error_msg}\")\n",
    "            return \"error\", error_msg\n",
    "\n",
    "    except APIConnectionError as e:\n",
    "        print(f\"❌ ERROR: Image API connection error: {e}\")\n",
    "        return \"error\", f\"Connection error: {e}\"\n",
    "    except RateLimitError as e:\n",
    "        print(f\"❌ ERROR: Image API rate limit exceeded: {e}\")\n",
    "        return \"error\", f\"Rate limit error: {e}\"\n",
    "    except APIStatusError as e:\n",
    "        print(f\"❌ ERROR: Image API status error: {e.status_code} - {e.response}\")\n",
    "        # Extract error message if possible\n",
    "        error_message = f\"API status error {e.status_code}\"\n",
    "        try:\n",
    "            error_details = e.response.json()\n",
    "            if 'error' in error_details and 'message' in error_details['error']:\n",
    "                error_message += f\": {error_details['error']['message']}\"\n",
    "        except:\n",
    "            pass # Ignore if response parsing fails\n",
    "        return \"error\", error_message\n",
    "    except Exception as e:\n",
    "        print(f\"❌ ERROR: Unexpected error during image operation: {e}\")\n",
    "        print(traceback.format_exc())\n",
    "        return \"error\", f\"Unexpected error: {e}\"\n",
    "\n",
    "print(\"✅ Prompt assembly and Image generation/editing functions defined.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "output_embedded_package_id": "1im5RH5qMde1d538iGndzPJSn-FAAtuHx"
    },
    "executionInfo": {
     "elapsed": 242331,
     "status": "ok",
     "timestamp": 1746370117319,
     "user": {
      "displayName": "Wei Bing Chuah",
      "userId": "12139668370354715299"
     },
     "user_tz": -480
    },
    "id": "Wmc_z_39seUn",
    "outputId": "fd1c962b-f8ed-432b-c88e-43e8ca657740"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output Path of Stratefy with Prompts exists in D:\\Self-Project\\LLM\\agent_ads\\pipeline_donwstream_outputs\\run_20250506_220147\\run_metadata_20250506_220147.json\n",
      "--- Loading pipeline data with generated prompts from: D:\\Self-Project\\LLM\\agent_ads\\pipeline_donwstream_outputs\\run_20250506_220147\\run_metadata_20250506_220147.json ---\n",
      "✅ Successfully loaded data.\n",
      "\n",
      "✅ Created run output directory: D:\\Self-Project\\LLM\\agent_ads\\pipeline_donwstream_outputs\\run_20250506_220227\n",
      "⚠️ Warning: Base reference image file not found at calculated path: D:\\Self-Project\\LLM\\agent_ads\\pipeline_upstream_outputs\\noodle_20250504_142856.jpeg\n",
      "\n",
      "Found 5 structured prompts. Assembling final prompts and generating/editing images...\n",
      "\n",
      "--- Processing Strategy 0 ---\n",
      "--- Calling Image Generation API (gpt-image-1) ---\n",
      "❌ Image API response format mismatch for generation.\n",
      "\n",
      "--- Processing Strategy 1 ---\n",
      "--- Calling Image Generation API (gpt-image-1) ---\n",
      "❌ Image API response format mismatch for generation.\n",
      "\n",
      "--- Processing Strategy 2 ---\n",
      "--- Calling Image Generation API (gpt-image-1) ---\n",
      "❌ Image API response format mismatch for generation.\n",
      "\n",
      "--- Processing Strategy 3 ---\n",
      "--- Calling Image Generation API (gpt-image-1) ---\n",
      "❌ Image API response format mismatch for generation.\n",
      "\n",
      "--- Processing Strategy 4 ---\n",
      "--- Calling Image Generation API (gpt-image-1) ---\n",
      "❌ Image API response format mismatch for generation.\n",
      "\n",
      "--- Image Generation/Editing/Saving Complete ---\n",
      "✅ Stored assembled prompts and image generation results (with paths) in pipeline_data.\n",
      "\n",
      "--- Generated Image Results ---\n",
      "\n",
      "Strategy 0:\n",
      "  Status: Error\n",
      "  Message: Image API response format mismatch for generation. Revised prompt (if available): N/A\n",
      "\n",
      "Strategy 1:\n",
      "  Status: Error\n",
      "  Message: Image API response format mismatch for generation. Revised prompt (if available): N/A\n",
      "\n",
      "Strategy 2:\n",
      "  Status: Error\n",
      "  Message: Image API response format mismatch for generation. Revised prompt (if available): N/A\n",
      "\n",
      "Strategy 3:\n",
      "  Status: Error\n",
      "  Message: Image API response format mismatch for generation. Revised prompt (if available): N/A\n",
      "\n",
      "Strategy 4:\n",
      "  Status: Error\n",
      "  Message: Image API response format mismatch for generation. Revised prompt (if available): N/A\n",
      "\n",
      "Successfully generated and saved 0 out of 5 images.\n",
      "\n",
      "✅ Successfully saved final pipeline metadata to: D:\\Self-Project\\LLM\\agent_ads\\pipeline_donwstream_outputs\\run_20250506_220227\\run_metadata_20250506_220227.json\n"
     ]
    }
   ],
   "source": [
    "# @title Step 5 & 6: Update Pipeline - Generate Images and Display Results\n",
    "\n",
    "# ** MODIFIED: Explicitly load the JSON containing generated prompts **\n",
    "pipeline_data = None\n",
    "\n",
    "# Make sure this filename matches the output from the previous cell run\n",
    "# Example: prompts_json_path = \"/content/drive/MyDrive/AI Imagery Marketing Tool/Colab Notebook/pipeline_upstream_outputs/output_with_prompts_20250503_032100.json\"\n",
    "# ** You might need to manually update this path based on the actual output filename **\n",
    "if 'output_path' in globals() and os.path.exists(output_path): # Check if path from previous cell exists\n",
    "    prompts_json_path = output_path\n",
    "    print(f\"Output Path of Stratefy with Prompts exists in {output_path}\")\n",
    "else:\n",
    "    # # Fallback to the user-provided path if the variable doesn't exist (e.g., running cells separately)\n",
    "    # prompts_json_dir = \"/content/drive/MyDrive/AI Imagery Marketing Tool/Colab Notebook/pipeline_upstream_outputs\" # <-- Path to JSON with prompts\n",
    "    # prompts_json_filename = \"output_with_prompts_20250504_144134.json\"\n",
    "    # prompts_json_path = os.path.join(prompts_json_dir, prompts_json_filename)\n",
    "    raise LookupError(\"Output of Strategy with Prompts Json File Not Found !!!\")\n",
    "\n",
    "print(f\"--- Loading pipeline data with generated prompts from: {prompts_json_path} ---\")\n",
    "try:\n",
    "    with open(prompts_json_path, 'r') as f:\n",
    "        pipeline_data = json.load(f)\n",
    "    print(f\"✅ Successfully loaded data.\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error loading JSON file '{prompts_json_path}': {e}\")\n",
    "    pipeline_data = None\n",
    "\n",
    "\n",
    "# Ensure functions are defined and clients/data are ready\n",
    "if ('assemble_final_prompt' in globals() and\n",
    "    'generate_image' in globals() and\n",
    "    pipeline_data and # Check if data was loaded successfully above\n",
    "    image_client and   # Check if image client is configured\n",
    "    PIPELINE_DOWNSTREAM_DIR): # Check if downstream dir is set\n",
    "\n",
    "    # --- Create Run-Specific Output Directory ---\n",
    "    run_timestamp = time.strftime('%Y%m%d_%H%M%S')\n",
    "    current_run_dir = os.path.join(PIPELINE_DOWNSTREAM_DIR, f\"run_{run_timestamp}\")\n",
    "    try:\n",
    "        os.makedirs(current_run_dir, exist_ok=True)\n",
    "        print(f\"\\n✅ Created run output directory: {current_run_dir}\")\n",
    "    except Exception as run_dir_e:\n",
    "        print(f\"❌ ERROR: Could not create run directory '{current_run_dir}': {run_dir_e}\")\n",
    "        raise SystemExit(\"Halting execution due to output directory creation failure.\") from run_dir_e\n",
    "\n",
    "\n",
    "    # Get the structured prompts generated in the previous cell\n",
    "    structured_prompts = pipeline_data.get(\"processing_context\", {}).get(\"generated_image_prompts\", [])\n",
    "    image_generation_results = [] # List to store results (url or error)\n",
    "    final_assembled_prompts = [] # List to store the assembled prompts used\n",
    "\n",
    "    # Get reference image info from the main JSON\n",
    "    user_inputs = pipeline_data.get(\"user_inputs\", {}) # Get user inputs dict\n",
    "    reference_image_info = user_inputs.get(\"image_reference\")\n",
    "    base_reference_image_path = None\n",
    "    if reference_image_info and IMAGE_INPUT_DIR: # Use IMAGE_INPUT_DIR for source\n",
    "        saved_image_filename = reference_image_info.get(\"saved_image_path\")\n",
    "        if saved_image_filename:\n",
    "            # Construct path assuming image is in the UPSTREAM output dir\n",
    "            base_reference_image_path = os.path.join(IMAGE_INPUT_DIR, saved_image_filename)\n",
    "            if not os.path.exists(base_reference_image_path):\n",
    "                 print(f\"⚠️ Warning: Base reference image file not found at calculated path: {base_reference_image_path}\")\n",
    "                 base_reference_image_path = None # Reset if not found\n",
    "            else:\n",
    "                 print(f\"✅ Found reference image: {base_reference_image_path}\")\n",
    "\n",
    "    if not structured_prompts:\n",
    "        print(\"⚠️ No structured prompts found in pipeline_data to generate images from.\")\n",
    "    else:\n",
    "        print(f\"\\nFound {len(structured_prompts)} structured prompts. Assembling final prompts and generating/editing images...\")\n",
    "\n",
    "        # Iterate through structured prompts\n",
    "        for structured_prompt_dict in structured_prompts:\n",
    "            strategy_index = structured_prompt_dict.get(\"source_strategy_index\", \"N/A\")\n",
    "            print(f\"\\n--- Processing Strategy {strategy_index} ---\")\n",
    "\n",
    "            # Pass user_inputs to assemble_final_prompt\n",
    "            final_prompt_str = assemble_final_prompt(structured_prompt_dict, user_inputs)\n",
    "            final_assembled_prompts.append({\"index\": strategy_index, \"prompt\": final_prompt_str}) # Store assembled prompt\n",
    "\n",
    "            if final_prompt_str.startswith(\"Error:\"):\n",
    "                print(f\"   Skipping image operation due to prompt assembly error: {final_prompt_str}\")\n",
    "                image_generation_results.append({\"index\": strategy_index, \"status\": \"error\", \"result\": final_prompt_str, \"saved_path\": None})\n",
    "                continue\n",
    "\n",
    "            # Step 4: Generate or Edit the image\n",
    "            aspect_ratio = structured_prompt_dict.get(\"aspect_ratio\", \"1:1\")\n",
    "            # Pass reference image path and RUN DIRECTORY to generate_image\n",
    "            status, result_data = generate_image(\n",
    "                final_prompt=final_prompt_str,\n",
    "                aspect_ratio=aspect_ratio,\n",
    "                client=image_client, # Use the separate image client\n",
    "                run_directory=current_run_dir, # Pass the specific run directory\n",
    "                strategy_index=strategy_index, # Pass index for filename\n",
    "                reference_image_path=base_reference_image_path # Pass the path\n",
    "            )\n",
    "\n",
    "            # ** MODIFIED: Handle URL download and saving **\n",
    "            saved_image_path = None # Initialize path for this iteration\n",
    "            if status == \"success\":\n",
    "                if result_data.startswith(\"http\"): # It's a URL from generation\n",
    "                    try:\n",
    "                        print(f\"   Downloading generated image from URL...\")\n",
    "                        image_response = requests.get(result_data, stream=True)\n",
    "                        image_response.raise_for_status() # Raise an exception for bad status codes\n",
    "                        # Create filename\n",
    "                        img_filename = f\"generated_image_strategy_{strategy_index}_{run_timestamp}.png\" # Or use original extension if detectable\n",
    "                        saved_image_path = os.path.join(current_run_dir, img_filename)\n",
    "                        with open(saved_image_path, \"wb\") as f:\n",
    "                            for chunk in image_response.iter_content(chunk_size=8192):\n",
    "                                f.write(chunk)\n",
    "                        print(f\"   Saved generated image to: {saved_image_path}\")\n",
    "                        result_data = saved_image_path # Update result to be the local path\n",
    "                    except requests.exceptions.RequestException as req_err:\n",
    "                         print(f\"❌ Error downloading image URL {result_data}: {req_err}\")\n",
    "                         status = \"error\"\n",
    "                         result_data = f\"Download error: {req_err}\"\n",
    "                    except IOError as io_err:\n",
    "                         print(f\"❌ Error saving downloaded image to {saved_image_path}: {io_err}\")\n",
    "                         status = \"error\"\n",
    "                         result_data = f\"File save error: {io_err}\"\n",
    "                    except Exception as download_err:\n",
    "                         print(f\"❌ Unexpected error during image download/save: {download_err}\")\n",
    "                         status = \"error\"\n",
    "                         result_data = f\"Unexpected download/save error: {download_err}\"\n",
    "                elif os.path.exists(result_data): # It's already a local path (from edit)\n",
    "                    saved_image_path = result_data # Already saved in generate_image\n",
    "                else: # Should not happen if status is success, but handle defensively\n",
    "                    print(f\"⚠️ Success status but invalid result data: {result_data}\")\n",
    "                    status = \"error\"\n",
    "                    result_data = \"Invalid success result data\"\n",
    "\n",
    "            image_generation_results.append({\"index\": strategy_index, \"status\": status, \"result_path\": saved_image_path, \"original_result\": result_data}) # Store path\n",
    "\n",
    "        print(\"\\n--- Image Generation/Editing/Saving Complete ---\")\n",
    "\n",
    "        # Store results back into the main data structure (now with local paths)\n",
    "        pipeline_data[\"processing_context\"][\"final_assembled_prompts\"] = final_assembled_prompts\n",
    "        pipeline_data[\"processing_context\"][\"generated_image_results\"] = image_generation_results\n",
    "        print(f\"✅ Stored assembled prompts and image generation results (with paths) in pipeline_data.\")\n",
    "\n",
    "        # --- Display Results ---\n",
    "        print(\"\\n--- Generated Image Results ---\")\n",
    "        num_success = 0\n",
    "        if image_generation_results:\n",
    "            for result_info in image_generation_results:\n",
    "                idx = result_info.get(\"index\", \"N/A\")\n",
    "                status = result_info.get(\"status\")\n",
    "                saved_path = result_info.get(\"result_path\")\n",
    "                original_result = result_info.get(\"original_result\")\n",
    "                print(f\"\\nStrategy {idx}:\")\n",
    "                if status == \"success\" and saved_path and os.path.exists(saved_path):\n",
    "                    num_success += 1\n",
    "                    print(f\"  Status: Success\")\n",
    "                    print(f\"  Saved Path: {saved_path}\")\n",
    "                    # Display the image using IPython.display from the saved path\n",
    "                    try:\n",
    "                        display(IPImage(filename=saved_path, width=256)) # Display smaller image\n",
    "                    except Exception as display_e:\n",
    "                        print(f\"  Error displaying image from path {saved_path}: {display_e}\")\n",
    "                elif status == \"success\": # Success reported but path invalid\n",
    "                     print(f\"  Status: Error (Post-processing)\")\n",
    "                     print(f\"  Message: Success reported but saved path is invalid: {saved_path}\")\n",
    "                     print(f\"  Original Result Data: {original_result}\")\n",
    "                else: # Status was 'error'\n",
    "                    print(f\"  Status: Error\")\n",
    "                    print(f\"  Message: {original_result}\") # Show original error message\n",
    "            print(f\"\\nSuccessfully generated and saved {num_success} out of {len(image_generation_results)} images.\")\n",
    "        else:\n",
    "            print(\"No image generation attempts were made.\")\n",
    "        # --- End Display ---\n",
    "\n",
    "\n",
    "        # ** MODIFIED: Save the final metadata JSON inside the run directory **\n",
    "        save_output = True # Set to False to disable saving\n",
    "        if save_output and current_run_dir: # Check if run dir was created\n",
    "            try:\n",
    "                # Save the metadata JSON within the run-specific directory\n",
    "                metadata_filename = f\"run_metadata_{run_timestamp}.json\"\n",
    "                output_path = os.path.join(current_run_dir, metadata_filename)\n",
    "                with open(output_path, 'w') as f:\n",
    "                    json.dump(pipeline_data, f, indent=2)\n",
    "                print(f\"\\n✅ Successfully saved final pipeline metadata to: {output_path}\")\n",
    "            except Exception as save_e:\n",
    "                print(f\"\\n❌ Error saving final metadata JSON file: {save_e}\")\n",
    "\n",
    "\n",
    "elif not pipeline_data:\n",
    "    print(\"⚠️ Cannot proceed: Pipeline data not loaded from the specified file.\")\n",
    "elif not image_client:\n",
    "    print(\"⚠️ Cannot proceed: Image generation client (OpenAI client) is not configured.\")\n",
    "elif not PIPELINE_DOWNSTREAM_DIR:\n",
    "     print(\"⚠️ Cannot proceed: Downstream output directory path is not valid.\")\n",
    "else:\n",
    "    print(\"⚠️ Cannot proceed: Required functions ('assemble_final_prompt', 'generate_image') not defined.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R6QjfSYs3l2x"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
